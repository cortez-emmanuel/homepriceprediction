```{r, message=FALSE, warning=FALSE}
require(knitr)
opts_chunk$set(eval=FALSE)
library(ISLR)
library(dplyr)
library(readr)
library(ggplot2)
library(GGally)
library(mosaic)
library(mosaic)
library(manipulate)
install.packages(ISLR)
library(ISLR)
install.packages(ISLR)
install.packages("ISLR")
library(dplyr)
install.package(dplyer)
install.packages(dplyer)
install.packages(c("dplyer", "readr", "ggplot2", "GGally", "mosaic", "manipulate"))
data(Auto)
install.packages(ISLR)
```{r, message=FALSE, warning=FALSE}
```{r, message=FALSE, warning=FALSE}
```{r, message=FALSE, warning=FALSE}
```{r, message=FALSE, warning=FALSE}
```{r, message=FALSE, warning=FALSE}
```{r, message=FALSE, warning=FALSE}
R.version
data(Auto)
```{r}
library(ISLR)
install.packages("ISLR")
data(Auto)
Auto
Auto %>%
slice(1:10)
Auto=read_csv("Auto.csv", na="?")
dim(Auto)
library(readr)
CONTROL <- read_csv("~/Graduate School/DEPLOY ADV ANALYTICS/Module 2 - Linear Regression/Project 1 - EDA/CONTROL.csv")
View(CONTROL)
(...)
stop_for_problems(CONTROL)
str(CONTROL)
dim(CONTROL)
head(CONTROL)
tail(CONTROL)
?subset()
CONTROL <- subset(CONTROL,select = -c(CONTROL, LOCALE)
CONTROL_del_LOCALE <- subset(CONTROL,select = -c(CONTROL, LOCALE)
CONTROL_del_LOCALE
CONTROL_del_LOCALE <- subset(CONTROL,select = -c(CONTROL, LOCALE)
CONTROL_del_LOCALE
CONTROL_del_LOCALE <- subset(CONTROL,select = -c(CONTROL, LOCALE))
CONTROL_del_LOCALE
str(CONTROL)
View(MERGED2018_19_PP)
library(readr)
CONTROL <- read_csv("~/Graduate School/DEPLOY ADV ANALYTICS/Module 2 - Linear Regression/Project 1 - EDA/MERGED2018_19_PP.csv")
# Records
dim(schools)
P1Data <- read_csv("~/Graduate School/DEPLOY ADV ANALYTICS/Module 2 - Linear Regression/Project 1 - EDA/MERGED2018_19_PP.csv")
# Records
dim(P1Data)
?pmax()
?pmax()
P1Data <- read_csv("~/Graduate School/DEPLOY ADV ANALYTICS/Module 2 - Linear Regression/Project 1 - EDA/MERGED2018_19_PP.csv")
Saratoga <- read.table("https://www.dropbox.com/s/9c0bbmuxlhdzc9c/Saratoga%20NY%20Homes.txt?dl=1",header=T,sep=",")
names(Saratoga)
str(Saratoga)
dim(Saratoga)
n = dim(Saratoga)[1]
n=nrow(Saratoga)
train = sample(1:n,size=floor(n*.70),replace=F)
length(train)
.70*n
train = sample(1:n,size=floor(n*.70),replace=F) # obs used multiple times so replacec is FALSE
length(train)
.70*n
n = nrow(Saratoga)
m1 = floor(n*.60)   # or ceiling(n*.60)
m2 = floor(n*.20) # or ceiling(n*.20)
RO = sample(1:n,size=n,replace=F) # this command permutes the indices 1 - n.
train = RO[1:m1]
valid = RO[(m1+1):(m1+m2+1)]
test = RO[(m1+m2+2):n]
length(train)
length(valid)
length(test)
1036+346+346
# this function is good to use as a template when working on projects for a repeatable process
PredAcc = function(y,ypred){
RMSEP = sqrt(mean((y-ypred)^2))
MAE = mean(abs(y-ypred))
MAPE = mean(abs(y-ypred)/y)*100
cat("RMSEP\n")
cat("===============\n")
cat(RMSEP,"\n\n")
cat("MAE\n")
cat("===============\n")
cat(MAE,"\n\n")
cat("MAPE\n")
cat("===============\n")
cat(MAPE,"\n\n")
return(data.frame(RMSEP=RMSEP,MAE=MAE,MAPE=MAPE))
}
home.lm1 = lm(Price~.,data=Saratoga[train,])
summary(home.lm1)
y = Saratoga$Price[valid] # see how well it performs
ypred = predict(home.lm1,newdata=Saratoga[valid,])
results = PredAcc(y,ypred)
results$RMSEP
results$MAE
results$MAPE
results
#We will now construct a simplified model using stepwise model selection to reduce the complexity of our base model.  We can then use our prediction accuracy function to compare the two models in terms of their predictive performance on the validation cases.
home.step = step(home.lm1)
#The stepwise reduced models has 8 less terms, thus we are estimating 8 less parameters, resulting in a simpler model than the full model.  It should be case that this simpler model has better predictive performance.  To see if this is the case, we again use our validation set and measure the predictive accuracy of this simpler model for the validation case response values.
ypred = predict(home.step,newdata=Saratoga[valid,])
results.step = PredAcc(y,ypred)
results
results.step
#As expected, the simpler model has better predictive performance than the larger, more complex, MLR model (though only slightly).   At this point we might decide our reduced model is the "best" MLR model we can develop for these data (which I highly doubt it is).  Thus we can get a final estimate of the predictive performance of this model for future observations by looking at the prediction accuracy for test cases.
ypred = predict(home.step,newdata=Saratoga[test,])
y = Saratoga$Price[test]
results.test = PredAcc(y,ypred)
kfold.MLR = function(fit,k=10,data=fit$model) {
sum.sqerr = rep(0,k)
sum.abserr = rep(0,k)
sum.pererr = rep(0,k)
y = fit$model[,1]
x = fit$model[,-1]
n = nrow(data)
folds = sample(1:k,nrow(data),replace=T)
for (i in 1:k) {
fit2 <- lm(formula(fit),data=data[folds!=i,])
ypred = predict(fit2,newdata=data[folds==i,])
sum.sqerr[i] = sum((y[folds==i]-ypred)^2)
sum.abserr[i] = sum(abs(y[folds==i]-ypred))
sum.pererr[i] = sum(abs(y[folds==i]-ypred)/y[folds==i])
}
cv = return(data.frame(RMSEP=sqrt(sum(sum.sqerr)/n),
MAE=sum(sum.abserr)/n,
MAPE=(sum(sum.pererr)/n)*100))
}
home.lm1 = lm(Price~.,data=Saratoga)
home.step = step(home.lm1)
results.full = kfold.MLR(home.lm1)
results.full
results.step = kfold.MLR(home.step)
results.step
PRESS = function(lm1){
lmi = lm.influence(lm1)
h = lmi$hat
e = resid(lm1)
PRESS = sum((e/(1-h))^2)
RMSEP = sqrt(PRESS/n)
return(data.frame(PRESS=PRESS,RMSEP=RMSEP))
}
home.lm1 = lm(Price~.,data=Saratoga)  # again fit using all of the full dataset
home.step = step(home.lm1)
press.full = PRESS(home.lm1)
press.step = PRESS(home.step)
press.full
press.step
# bootstrapping with 100 samples
bootols.cv = function(fit,B=100,data=fit$model) {
yact=fit$fitted.values+fit$residuals
ASR=mean(fit$residuals^2)
AAR=mean(abs(fit$residuals))
APE=mean(abs(fit$residuals)/yact)
boot.sqerr=rep(0,B)
boot.abserr=rep(0,B)
boot.perr=rep(0,B)
y = fit$model[,1]
x = fit$model[,-1]
n = nrow(data)
for (i in 1:B) {
sam=sample(1:n,n,replace=T)
samind=sort(unique(sam))
temp=lm(formula(fit),data=data[sam,])
ypred=predict(temp,newdata=data[-samind,])
boot.sqerr[i]=mean((y[-samind]-ypred)^2)
boot.abserr[i]=mean(abs(y[-samind]-ypred))
boot.perr[i]=mean(abs(y[-samind]-ypred)/y[-samind])
}
ASRo=mean(boot.sqerr)
AARo=mean(boot.abserr)
APEo=mean(boot.perr)
OPsq=.632*(ASRo-ASR)
OPab=.632*(AARo-AAR)
OPpe=.632*(APEo-APE)
RMSEP=sqrt(ASR+OPsq)
MAEP=AAR+OPab
MAPEP=(APE+OPpe)*100
cat("RMSEP\n")
cat("===============\n")
cat(RMSEP,"\n\n")
cat("MAE\n")
cat("===============\n")
cat(MAEP,"\n\n")
cat("MAPE\n")
cat("===============\n")
cat(MAPEP,"\n\n")
return(data.frame(RMSEP=RMSEP,MAE=MAEP,MAPE=MAPEP))
}
home.lm1 = lm(Price~.,data=Saratoga)
results.boot = bootols.cv(home.lm1,B=100)
results.boot = bootols.cv(home.lm1,B=1000) # increasing the # of bootstrap samples (B = 1000)
results.boot = bootols.cv(home.lm1,B=5000) # increasing the # of bootstrap samples (B = 5000)
library(doParallel)
library(foreach)
library(iterators)
library(parallel)
library(doParallel)
detectCores() #detect the # of cores, you'll want to register fewer than your available cores.  I have 8, so I'll register 5
# Create a cluster object and then register:
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
library(caret)
# Split the data into training and test set
set.seed(123)
training.samples<-createDataPartition(Saratoga$Price,
p = .7, #% of data going to training
list = F)
train.data  <- Saratoga[training.samples, ]
test.data <- Saratoga[-training.samples, ]
set.seed(123)
cv<- trainControl(method = "cv", number = 5) #5 fold CV
repeatedcv<-trainControl(method = 'repeatedcv',number = 10, repeats=5)#10 fold CV, repeated 5 times
lmCVFit<-train(Price~.,
data=train.data,
method = "lm",  #Available models: https://topepo.github.io/caret/available-models.html
trControl = cv,
metric = "RMSE")
#Examine the model
summary(lmCVFit)
#Examine model predictions
lmCVFit
#examine model predictions for each fold.
lmCVFit$resample
#standard deviation around the RMSE
sd(lmCVFit$resample$RMSE)
varImp(lmCVFit) #This function tells you which variables are most important (it works on other algorithms as well) # good for black box models like ANN
plot (varImp(lmCVFit))
#Evaluate the performance on the test dataset
predictedVal<-predict(lmCVFit,test.data)
modelvalues<-data.frame(obs = test.data$Price, pred=predictedVal)
defaultSummary(modelvalues)
setwd("~/Graduate School/DEPLOY ADV ANALYTICS/Module 3 - Resampling Methods/Extra Credit")
library(readr)
King_County_Homes_train_ <- read_csv("King County Homes (train).csv")
View(King_County_Homes_train_)
dim(King_County_Homes_train_)
str(King_County_Homes_train_)
df <- as.data.frame(King_County_Homes_train_)
